{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "NqAP58wXzzvz",
        "outputId": "614b678b-9629-4909-fb0c-e5d968bfbdee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-d12eaf30d68e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#use Numpy arrays to input desired input and desired output parameters into the model using a fit function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#same principle as in sci-kit learn library\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_x_y_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m     \u001b[0m_check_data_cardinality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'NoneType'"
          ]
        }
      ],
      "source": [
        "#input & defining of the training set in AI\n",
        "#import keras library\n",
        "import keras\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "\n",
        "#basic principles of the training data set, define input parameters and desired output parameters\n",
        "input_tensor = layers.Input(shape=(784,))\n",
        "x = layers.Dense(32, activation = 'relu')(input_tensor)\n",
        "output_tensor = layers.Dense(10, activation = 'softmax')(x)\n",
        "model = models.Model(inputs = input_tensor, outputs = output_tensor)\n",
        "\n",
        "#can also be done with the sequential class\n",
        "\n",
        "#deep learning is configured during optimisation process with an optimiser and a loss function, both must be defined for the model to use them\n",
        "#need to define parameters that will be used to monitor the process of learning\n",
        "from keras import optimizers\n",
        "model.compile(optimizer = optimizers.RMSprop(lr=0.001),\n",
        "              loss='mse',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#use Numpy arrays to input desired input and desired output parameters into the model using a fit function\n",
        "#same principle as in sci-kit learn library\n",
        "#there is no code defining target_tensor so the line below doesn't work yet\n",
        "model.fit(input_tensor, target_tensor, batch_size=28, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing IMDB database with movie ratings, the option to extract this dataset already exists within\n",
        "from keras.datasets import imdb\n",
        "(train_data, train_labels),(test_data, test_labels) = imdb.load_data(num_words=10000)\n",
        "#due to file size only keep 10k words\n",
        "#train_data and test_data are lists of reviews, each review is a list of words\n",
        "#labels are assigned to categorise reviews into positive and negative as numbers, 0 is negative review, 1 is a positive\n",
        "train_data[0]\n",
        "test_labels[0]\n",
        "max([max(sequence) for sequence in train_data])\n",
        "\n",
        "#you can use a loop to read one of the reviews in English\n",
        "word_index = imdb.get_word_index() #assign index to every word\n",
        "reverse_word_index = dict(\n",
        "    [(value, key) for (key, value) in word_index.items()])\n",
        "#reverse index to get words\n",
        "#fetch the first review in the train_data using the reversed_index\n",
        "decoded_review = ' '.join([reverse_word_index.get(i-3, '?') for i in train_data[0]])\n",
        "print(decoded_review)\n",
        "\n",
        "#before using data within the learning model you must convert it into tensors or vectors of 0s and 1s\n",
        "#you can turn it into tensors of same dimesions, with integers (samples, word_index), then use first layer of the learning network to \n",
        "#convert tensors into integers with Embedding\n",
        "\n",
        "#or as shown below\n",
        "#you can encode vectors of 0s and 1s, separated by ',' and depending on index e.g. [6,4] it will only contain 0s at all positions except index 6 and 4\n",
        "#index of 6 and 4 will contain 1s\n",
        "#use Dense layer to convert vectors\n",
        "import numpy as np\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "  results = np.zeros((len(sequences), dimension))\n",
        "  for i, sequence in enumerate(sequences):\n",
        "    results[i, sequence] = 1,\n",
        "  return results\n",
        "\n",
        "#creates vector with dimensions(len(seuqences), 10000)\n",
        "\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)\n",
        "\n",
        "#change labels into vectors as well\n",
        "y_train = np.array(train_labels).astype('float32')\n",
        "y_test = np.array(test_labels).astype('float32')\n",
        "\n",
        "#simple vectors can be run on model Dense with function 'relu\n",
        "#simple model with 2 layers and 16 nodes per layer, process the data with relu function\n",
        "#you have to define number of nodes per layer and number of layers per model\n",
        "#3rd layter uses sigmoid function that can squeeze/convert all values into  0s and 1s and generate predictive model, which determines positive review\n",
        "\n",
        "\n",
        "#building the model\n",
        "from keras import models\n",
        "from keras import layers\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation = 'relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(16, activation = 'relu'))\n",
        "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "#now need to define loss function and optimilisation \n",
        "#loss of function for this example would be binary_crossentropy \n",
        "#which fits the categorisation of reviews in binary manner and works well with sigmoid function\n",
        "#mean_squared_error is another loss function but it works best with probability outputs\n",
        "#mean_squared_error calculates the distance of desired output vs achieved output\n",
        "\n",
        "#now choosing optimilisation via rmsprop, choosing binary_crossentropy as loss of function, and checking the model with accurracy\n",
        "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['accuracy',])\n",
        "#these are in-built in Keras, you can also use optimiser and loss function from a class, or a function \n",
        "\n",
        "#now exclude data to prepare a validation dataset for this model\n",
        "x_val = x_train[:10000]\n",
        "partial_x_train = x_train[10000:]\n",
        "\n",
        "y_val = y_train[:10000]\n",
        "partial_y_train = y_train[10000:]\n",
        "\n",
        "#the model will be run across 20 epochs (iterations) with 512 samples, while \n",
        "#in the meantime the model will be validated against 10k samples set aside above\n",
        "#validation data set is defined in a separate argument\n",
        "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['accuracy',])\n",
        "history = model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val))\n",
        "\n",
        "#takes 20 s with minor interruptions as the model is being validated\n",
        "#having closer look at the history variable which shows the process of the training of the model\n",
        "\n",
        "history_dict = history.history\n",
        "history_dict.keys()\n",
        "[u'acc', u'loss', u'val_acc', u'val_loss']\n",
        "\n",
        "#plotting now loss function and validation of the model\n",
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc)+1)\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Showing loss of validation and training of the model')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "#plotting accuracy of model training and validation\n",
        "\n",
        "plt.clf()\n",
        "acc_values = history_dict['accuracy']\n",
        "val_acc_values = history_dict['val_accuracy']\n",
        "\n",
        "plt.plot(epochs, acc_values, 'bo', label='Accurracy of training')\n",
        "plt.plot(epochs, val_acc_values, 'b', label='Validation accuracy')\n",
        "plt.title('Showing the accuracy of validation and training of the model')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#training now data set again since 20 epochs was too much and it caused overfitting of the model\n",
        "#now training with 4 epochs\n",
        "model = model.Sequential()\n",
        "model.add(layers.Dense(16, activation = 'relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(16, activation = 'relu'))\n",
        "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
        "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['accuracy',])\n",
        "\n",
        "history = model.fit(x_train,\n",
        "                    y_train,\n",
        "                    epochs=4,\n",
        "                    batch_size=512)\n",
        "\n",
        "results = model.evaluate(x_test, y_test)\n",
        "\n",
        "results\n",
        "#should show accuracy of ~80%\n",
        "#can now use the model to predict\n",
        "model.predict(x_test)\n",
        "\n",
        "#can also use loss function 'mse', activation function of tanh, and different number of nodes per layer\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlxGMmQ_80zX",
        "outputId": "481ccf13-abd7-48cf-9904-b03e07c8aa90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n",
            "Epoch 1/20\n",
            "30/30 [==============================] - 3s 62ms/step - loss: 0.5236 - accuracy: 0.7926 - val_loss: 0.4008 - val_accuracy: 0.8660\n",
            "Epoch 2/20\n",
            "30/30 [==============================] - 2s 56ms/step - loss: 0.3215 - accuracy: 0.8990 - val_loss: 0.3155 - val_accuracy: 0.8841\n",
            "Epoch 3/20\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.2349 - accuracy: 0.9263 - val_loss: 0.2812 - val_accuracy: 0.8885\n",
            "Epoch 4/20\n",
            "30/30 [==============================] - 1s 37ms/step - loss: 0.1847 - accuracy: 0.9385 - val_loss: 0.2813 - val_accuracy: 0.8861\n",
            "Epoch 5/20\n",
            "30/30 [==============================] - 1s 37ms/step - loss: 0.1483 - accuracy: 0.9523 - val_loss: 0.2796 - val_accuracy: 0.8871\n",
            "Epoch 6/20\n",
            "30/30 [==============================] - 1s 37ms/step - loss: 0.1202 - accuracy: 0.9633 - val_loss: 0.2893 - val_accuracy: 0.8871\n",
            "Epoch 7/20\n",
            "30/30 [==============================] - 1s 37ms/step - loss: 0.0988 - accuracy: 0.9709 - val_loss: 0.3031 - val_accuracy: 0.8842\n",
            "Epoch 8/20\n",
            "30/30 [==============================] - 1s 37ms/step - loss: 0.0799 - accuracy: 0.9775 - val_loss: 0.3629 - val_accuracy: 0.8749\n",
            "Epoch 9/20\n",
            "30/30 [==============================] - 1s 37ms/step - loss: 0.0653 - accuracy: 0.9825 - val_loss: 0.3493 - val_accuracy: 0.8788\n",
            "Epoch 10/20\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.0515 - accuracy: 0.9886 - val_loss: 0.3813 - val_accuracy: 0.8755\n",
            "Epoch 11/20\n",
            "30/30 [==============================] - 1s 37ms/step - loss: 0.0411 - accuracy: 0.9907 - val_loss: 0.4176 - val_accuracy: 0.8750\n",
            "Epoch 12/20\n",
            "30/30 [==============================] - 1s 39ms/step - loss: 0.0325 - accuracy: 0.9937 - val_loss: 0.4393 - val_accuracy: 0.8714\n",
            "Epoch 13/20\n",
            "30/30 [==============================] - 1s 37ms/step - loss: 0.0236 - accuracy: 0.9960 - val_loss: 0.4706 - val_accuracy: 0.8706\n",
            "Epoch 14/20\n",
            "30/30 [==============================] - 1s 37ms/step - loss: 0.0184 - accuracy: 0.9974 - val_loss: 0.5372 - val_accuracy: 0.8628\n",
            "Epoch 15/20\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.0141 - accuracy: 0.9986 - val_loss: 0.5547 - val_accuracy: 0.8703\n",
            "Epoch 16/20\n",
            " 7/30 [======>.......................] - ETA: 0s - loss: 0.0084 - accuracy: 0.9994"
          ]
        }
      ]
    }
  ]
}