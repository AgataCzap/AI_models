{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ovpZyIhNIgoq"
      },
      "source": [
        "___\n",
        "\n",
        "<a href='http://www.pieriandata.com'> <img src='../Pierian_Data_Logo.png' /></a>\n",
        "___\n",
        "# Text Generation with Neural Networks\n",
        "\n",
        "In this notebook we will create a network that can generate text, here we show it being done character by character. Very awesome write up on this here: http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
        "\n",
        "We organized the process into \"steps\" so you can easily follow along with your own data sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "00mwlQwloO5_"
      },
      "outputs": [],
      "source": [
        "# GOOGLE COLLAB USERS ONLY\n",
        "%tensorflow_version 2.x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "WBd69MDEm4rF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "sDbPtshnm4rL"
      },
      "outputs": [],
      "source": [
        "# IGNORE THE CONTENT OF THIS CELL\n",
        "# tf.compat.v1.disable_eager_execution()\n",
        "#import main libraries, import text and understand characters in text files\n",
        "#import tensorflow as tf, rarely done but here needed, grab the data,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "kycWuRI9oaSP"
      },
      "outputs": [],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "apj1Chkdm4rS"
      },
      "source": [
        "## Step 1: The Data\n",
        "\n",
        "You can grab any free text you want from here: https://www.gutenberg.org/\n",
        "\n",
        "We'll choose all of shakespeare's works (which we have already downloaded for you), mainly for two reasons:\n",
        "\n",
        "1. Its a large corpus of text, its usually recommended you have at least a source of 1 million characters total to get realistic text generation.\n",
        "\n",
        "2. It has a very distinctive style. Since the text data uses old style english and is formatted in the style of a stage play, it will be very obvious to us if the model is able to reproduce similar results."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2 main reasons, large work of text, >1 mil characters, and more importantly Shakespeare has distinct style and play, names are capitalized, someone enters or exits, undertsand \n",
        "create path to file, where text file is, notebook in same location as the file\n",
        "read in with open(path_file, 'r').read() = read only mode.\n",
        "Has spaces, \\n, which stands for new line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "pD_55cOxLkAb"
      },
      "outputs": [],
      "source": [
        "path_to_file = 'shakespeare.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "aavnuByVymwK"
      },
      "outputs": [],
      "source": [
        "text = open(path_to_file, 'r').read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Duhg9NrUymwO"
      },
      "outputs": [],
      "source": [
        "print(text[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(text(4500:4800))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(text(140500:141500))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Clearly, distinct spaces here, new line, some number for a sonet and another sonet. network will be able to learn this.\n",
        "Later will have plays, Helena speaks, expand., 500 more characters, King etc.\n",
        "Well structured text.\n",
        "Understand unique characters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XXUmR627m4rd"
      },
      "source": [
        "### Understanding unique characters"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get uniqye characters with sorted(set(text)), assign to vocab, new line, space, !, some numbers, capital and lowercase letters, punctuation ad\n",
        "braces. 84 unique characters, keep in mind when designing last Dense layer.\n",
        "Make sure to import tensorflow as tf and use correct file path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "IlCgQBRVymwR"
      },
      "outputs": [],
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(vocab)\n",
        "len(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rNnrKn_lL-IJ"
      },
      "source": [
        "## Step 2: Text Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LFjSVAlWzf-N"
      },
      "source": [
        "### Text Vectorization\n",
        "\n",
        "We know a neural network can't take in the raw string data, we need to assign numbers to each character. Let's create two dictionaries that can go from numeric index to character and character to numeric index."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In part 2, text processing, vectorising actual data, making dictionaries that network can go back and forth between, and also creating batches.\n",
        "Previously we went ahead and created a set of all characters, sorted list.\n",
        "Want to assign a number to each character."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for pair in enumerate(vocab):\n",
        "    print(pair)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Created loop, assigned number to every character, tuples.\n",
        "Use this logic to generate a dictionary, that goes from character to certain index code.\n",
        "Use character to index = {char:ind for ind, char in enumarate(vocab) - created dictionary, all charatcers are now the keys for index values.}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "IalZLbvOzf-F"
      },
      "outputs": [],
      "source": [
        "char_to_ind = {u:i for i, u in enumerate(vocab)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "fmmP5iCwm4rp"
      },
      "outputs": [],
      "source": [
        "char_to_ind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "30ZYaWAOm4rt"
      },
      "outputs": [],
      "source": [
        "ind_to_char = np.array(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "_6JPOWwJm4rz"
      },
      "outputs": [],
      "source": [
        "ind_to_char"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "char_to_ind['H'] #returs 33"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ind_to_char = np.array(vocab)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reverse, a vocab is a simple index call on the array, so use ind_to_char[33] get H. \n",
        "2 objects that easily to between characters, for all the text, encode it to integers.\n",
        "Create encoded ytext with np.array([char_to_ind[c] for c in text]) - read text before, have entire text, encoded all of the text as an array.\n",
        "Will take some time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "3fhOqV0lm4r2"
      },
      "outputs": [],
      "source": [
        "encoded_text = np.array([char_to_ind[c] for c in text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "axOX7rFom4r5"
      },
      "outputs": [],
      "source": [
        "encoded_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoded_text.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Goes from 0 ... to 39, check shape, over 5 mill characters, that is enough for good results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tZfqhkYCymwX"
      },
      "source": [
        "We now have a mapping we can use to go back and forth from characters to numerics."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you had sample text of 20 indexes, first item in the sonnet, look at its encoded form, see all digits related to it.\n",
        "Go from char to index encoding and from index to character."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "tFs1Uza-m4r9"
      },
      "outputs": [],
      "source": [
        "sample = text[:20]\n",
        "sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "gIqUCK5Am4sB"
      },
      "outputs": [],
      "source": [
        "encoded_text[:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bbmsf23Bymwe"
      },
      "source": [
        "## Step 3: Creating Batches\n",
        "\n",
        "Overall what we are trying to achieve is to have the model predict the next highest probability character given a historical sequence of characters. Its up to us (the user) to choose how long that historic sequence. Too short a sequence and we don't have enough information (e.g. given the letter \"a\" , what is the next character) , too long a sequence and training will take too long and most likely overfit to sequence characters that are irrelevant to characters farther out. While there is no correct sequence length choice, you should consider the text itself, how long normal phrases are in it, and a reasonable idea of what characters/words are relevant to each other."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In step 3 we create the batchers, how text seq are organised and shift them 1 character/step forward.\n",
        "Understand how Tensorflow generates batches use built in dataset class object to generate batches, and shuffle those batches.\n",
        "We discovered how to take string data and encode it to have string assigned to it, and get bactches to train model on.\n",
        "How long should sequence be?\n",
        "Grab first 500 characters, general structure of the sonnet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "pAvUYFk7m4sF"
      },
      "outputs": [],
      "source": [
        "print(text[:500])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save single line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "D45OYgOfm4sJ"
      },
      "outputs": [],
      "source": [
        "line = \"From fairest creatures we desire increase\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "How long is that line? 41 characters, make sure training sequences are long enough so that model learns general patterns.\n",
        "Shakespeare has specific structure due to the iambic pentameter, things rhyme to every other line, more or less.\n",
        "Increase and decease, die and memory etc. Should have at least 3 lines to understand that structure, lines = ''' - triple quotes so multiline\n",
        "41*3 = 133, choose seq length of 120 characters, smaller set of lines, seq length will depend on the structure of the text and what you want to\n",
        "achieve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "7dKiEVN8m4sL"
      },
      "outputs": [],
      "source": [
        "len(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "olX67f6-m4sP"
      },
      "outputs": [],
      "source": [
        "part_stanza = \"\"\"From fairest creatures we desire increase,\n",
        "  That thereby beauty's rose might never die,\n",
        "  But as the riper should by time decease,\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "qal7MQnqm4sQ"
      },
      "outputs": [],
      "source": [
        "len(part_stanza)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hgsVvVxnymwf"
      },
      "source": [
        "### Training Sequences\n",
        "\n",
        "The actual text data will be the text sequence shifted one character forward. For example:\n",
        "\n",
        "Sequence In: \"Hello my nam\"\n",
        "Sequence Out: \"ello my name\"\n",
        "\n",
        "\n",
        "We can use the `tf.data.Dataset.from_tensor_slices` function to convert a text vector into a stream of character indices."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Choose seq length of 120, calculate total number of sequeces in text len(text)/seq_len, use // classic true division, round off\n",
        "(seq_len+1), due to index 0, 45K sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "0UHJDA39zf-O"
      },
      "outputs": [],
      "source": [
        "seq_len = 120"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "7VRSK4cOm4sZ"
      },
      "outputs": [],
      "source": [
        "total_num_seq = len(text)//(seq_len+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "xtW0jbbvm4sc"
      },
      "outputs": [],
      "source": [
        "total_num_seq"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use tensorflow object, data.Dataset.from_tesor_slices(on encoded text not the original text)\n",
        "Will run for a both, special tensor sliced dataset,. char_dataset. use tab, manny commands, use batch method, converts sequences into batches\n",
        "that can be fed onto the model for training.\n",
        "What doess that look like?\n",
        "for item in char_dataset.take - method, creates dataset with at most count elements, say 500, print items. convert to numpy()\n",
        "see encoded text, use ind_to_char, printeed character by character, inside that large character dataset, create sequences from it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ciatnowvm4se"
      },
      "outputs": [],
      "source": [
        "# Create Training Sequences\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n",
        "\n",
        "for i in char_dataset.take(500):\n",
        "     print(ind_to_char[i.numpy()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-ZSYAcQV8OGP"
      },
      "source": [
        "The **batch** method converts these individual character calls into sequences we can feed in as a batch. We use seq_len+1 because of zero indexing. Here is what drop_remainder means:\n",
        "\n",
        "drop_remainder: (Optional.) A `tf.bool` scalar `tf.Tensor`, representing\n",
        "    whether the last batch should be dropped in the case it has fewer than\n",
        "    `batch_size` elements; the default behavior is not to drop the smaller\n",
        "    batch.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now create sequences in batches, use the batch method on char_dataset, this batch combines consecutive sequences into a batches, that can be fed\n",
        "onto the network. Define first item as seq_len + 1, due to 0 index and drop remainder = True.\n",
        "We did classic division, use true division, get decimals, not perfect fit, make sense, not expected perfect fit, characters at the end.\n",
        "Drip remainder, drops last characters, 119 characters out of 5 million characters is not big deal.\n",
        "Get sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "l4hkDU3i7ozi"
      },
      "outputs": [],
      "source": [
        "sequences = char_dataset.batch(seq_len+1, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UbLcIPBj_mWZ"
      },
      "source": [
        "Now that we have our sequences, we will perform the following steps for each one to create our target text sequences:\n",
        "\n",
        "1. Grab the input text sequence\n",
        "2. Assign the target text sequence as the input text sequence shifted by one step forward\n",
        "3. Group them together as a tuple"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Perform following steps to grab target sequences.\n",
        "Grab seq, assign target seq and shift one char forwarsd, group together as tuple.\n",
        "Create a function:\n",
        "def create_seq_targets(seq):\n",
        "    input_txt = seq[:-1] - from beginning except last char\n",
        "    target_txt = seq[1:] - shifted 1 char forward\n",
        "    return input_txt, target_txt\n",
        "\n",
        "from hello my nam to ello my name, shifted by 1 char step into the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "9NGu-FkO_kYU"
      },
      "outputs": [],
      "source": [
        "def create_seq_targets(seq):\n",
        "    input_txt = seq[:-1]\n",
        "    target_txt = seq[1:]\n",
        "    return input_txt, target_txt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Map to all sequences, map across the entire dataset, sequences.map(create_seq_targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "HszljTg8m4so"
      },
      "outputs": [],
      "source": [
        "dataset = sequences.map(create_seq_targets)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For every pair of input and target seqs, print input text as numpy array, \n",
        "print index of characters, join that, print new line for clarity and print target text as numpy.\n",
        "Print and join that with index to char, for target text.\n",
        "\n",
        "Shows the first batch, 0 111 to 76 75, first sequences, what it looks like as characters below, network sees numbers only.\n",
        "Then next sequence from 111 to 76 75 1, where 1 is white space, extrac white space, instead of T. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "JkPa7AMrm4sq"
      },
      "outputs": [],
      "source": [
        "for input_txt, target_txt in  dataset.take(1):\n",
        "    print(input_txt.numpy())\n",
        "    print(''.join(ind_to_char[input_txt.numpy()]))\n",
        "    print('\\n')\n",
        "    print(target_txt.numpy())\n",
        "    # There is an extra whitespace!\n",
        "    print(''.join(ind_to_char[target_txt.numpy()]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MJdfPmdqzf-R"
      },
      "source": [
        "### Generating training batches\n",
        "\n",
        "Now that we have the actual sequences, we will create the batches, we want to shuffle these sequences into a random order, so the model doesn't overfit to any section of the text, but can instead generate characters given any seed text."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have sequences, generate batches, choose 128 batches, make sure to shuffle, so modern does not learn particular ordering of text,\n",
        "but get random text and learn from it/\n",
        "dataset = dataset.shuffle() to avoid shuffling entire text in memory use buffer_size = 10K, take 10K elements and shuffle them around, if\n",
        "you don't specify buffer size, wil shuffle entire dataset and cause error, will take more memory, drop remainder = True.\n",
        "Now should have dataset ready to go.\n",
        "Different types of sequences, training and targe, both sizes of tuple (128, 128).\n",
        "One of the most complex steps, 1 seq of encoded numbers that relates to a string, target is that sequence shifted by 1 character, more complex.\n",
        "Text must be encoded as numbers and have index to char and char to index. Seq is 128 character long, input and target sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "p2pGotuNzf-S"
      },
      "outputs": [],
      "source": [
        "# Batch size\n",
        "batch_size = 128\n",
        "\n",
        "# Buffer size to shuffle the dataset so it doesn't attempt to shuffle\n",
        "# the entire sequence in memory. Instead, it maintains a buffer in which it shuffles elements\n",
        "buffer_size = 10000\n",
        "\n",
        "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "gmcCALymm4su"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r6oUuElIMgVx"
      },
      "source": [
        "## Step 4: Creating the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m8gPwEjRzf-Z"
      },
      "source": [
        "We will use an LSTM based model with a few extra features, including an embedding layer to start off with and **two** LSTM layers. We based this model architecture off the [DeepMoji](https://deepmoji.mit.edu/) and the original source code can be found [here](https://github.com/bfelbo/DeepMoji).\n",
        "\n",
        "The embedding layer will serve as the input layer, which essentially creates a lookup table that maps the numbers indices of each character to a vector with \"embedding dim\" number of dimensions. As you can imagine, the larger this embedding size, the more complex the training. This is similar to the idea behind word2vec, where words are mapped to some n-dimensional space. Embedding before feeding straight into the LSTM usually leads to more realisitic results."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating the model by setting up loss function, simple model with 3 layers, witj 1 neuron per vocab character. \n",
        "vocab_size is length of vocab or unique characters, 84\n",
        "embedding dimension for embedding layer, choose 64 as same range as vocab_size, not too large, as some features discovered then won't be useful\n",
        "single layer with rnn_neurons, many , 1026 or 1024, single layer with a lot of neurons\n",
        "choose multiple layers, LSTM and GRU stacked on top of each other, choose simple model that can produce realistic results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "zHT8cLh7EAsg"
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embed_dim = 64\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_neurons = 1026"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Atb060h5m4s0"
      },
      "source": [
        "Now let's create a function that easily adapts to different variables as shown above."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "import all layers, with LSTM, Dense, Embedding, Dropout and Gru\n",
        "As labels are one hot encoded, use sparse categorical crossentropy, instead of categorical crossentropy.\n",
        "link, check it out, also use help on it, from logits = False as one hot encoded.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "YeRlEXgym4s1"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM,Dense,Embedding,Dropout,GRU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FcMbIy-xj-w-"
      },
      "source": [
        "### Setting up Loss Function\n",
        "\n",
        "For our loss we will use sparse categorical crossentropy, which we can import from Keras. We will also set this as logits=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "VoFVGKlNkJfW"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.losses import sparse_categorical_crossentropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "sblCzZoslZKH"
      },
      "outputs": [],
      "source": [
        "help(sparse_categorical_crossentropy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y5N4Qxbij5gY"
      },
      "source": [
        "https://datascience.stackexchange.com/questions/41921/sparse-categorical-crossentropy-vs-categorical-crossentropy-keras-accuracy"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Wrap it, taking y_true and y_pred, just as the original does, return sparse categorical function(y_true, y_pred, logits=T)\n",
        "Create custom loss function, specify the parameter to be True as default is false."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "FrOOK61Olm1C"
      },
      "outputs": [],
      "source": [
        "def sparse_cat_loss(y_true,y_pred):\n",
        "  return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save model in function, as you change dataset, you can easily adapt your model.\n",
        "Function takes in vocab size, embedding  dimensions, number of neurons RNN andd batch size.\n",
        "model starts witj Sequential\n",
        "\n",
        "add layers 1st is Embedding, embedds into some amount of dimensions, pass vocab size, input_dimensions, embedding dimensions as output_dimensions\n",
        "then batch input shape **kwargs argument, batch_size by None\n",
        "\n",
        "add GRU layer, with rnn_neuron number, can expand, a couple of parameters, can specify droput call within, set return_sequebces = True and stateful = True, we need to return last output in output seq or the full sequence, we are dealing with seqs shifted over, see history\n",
        "stateful = True, last state of each sample at index i in a batch will be used as initial state for the sample index i in the following batch, keep current state.\n",
        "recurrent_initializer = actual initializer of weight values for RNN, set to, several string codes, default is glorot uniform and recurrent as orthogonal, but recurrent performs better as glorot uniform too, name of person who developed it, typically named last name, he still works in DeepMind\n",
        "\n",
        "Finally add Dense layer, just the vocab size\n",
        "\n",
        "Compile model with adam optimizer and our loss function, pass variable than call the function, had to edit the function to specify logist=True,\n",
        "cannot do that by calling, will be default setting instead that is False.\n",
        "\n",
        "return the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "MtCrdfzEI2N0"
      },
      "outputs": [],
      "source": [
        "def create_model(vocab_size, embed_dim, rnn_neurons, batch_size):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, embed_dim,batch_input_shape=[batch_size, None]))\n",
        "    model.add(GRU(rnn_neurons,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'))\n",
        "    # Final Dense Layer to Predict\n",
        "    model.add(Dense(vocab_size))\n",
        "    model.compile(optimizer='adam', loss=sparse_cat_loss) \n",
        "    return model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use function to create the model, use your variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "wwsrpOik5zhv"
      },
      "outputs": [],
      "source": [
        "model = create_model(\n",
        "  vocab_size = vocab_size,\n",
        "  embed_dim=embed_dim,\n",
        "  rnn_neurons=rnn_neurons,\n",
        "  batch_size=batch_size)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "View the model, see your settings.\n",
        "Shapes are 128, None, 64 - batch_size, later will be changed when model is reloaded, and batch size will be 1.\n",
        "Check model, without any training to get random characters to check it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "liXuTFYMm4s6"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LJL0Q0YPY6Ee"
      },
      "source": [
        "## Step 5: Training the model\n",
        "\n",
        "Let's make sure everything is ok with our model before we spend too much time training! Let's pass in a batch to confirm the model currently predicts random characters without any training.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training model, show example of how to train the model, over 3 mill parameters, requires proper GPU and time, show how to load the model.\n",
        "Also show how to check if model works correctly, run model with no training, except random characters.\n",
        "Whenn calling model summary, 3 mill parameters, need a proper GPU and time to train, requires Kuda and GPU.\n",
        "Use Google colab, upload notebook, file - upload notebook, then choose file.\n",
        "Upload textdata - expand, files, make sure it is connnected to runtime, upload, shakespeare, or upload both model and text\n",
        "Text and trained model, load up later on, make sure to use tensorflow 2 so %tensorflow_version 2.x, hit runtime and run all, set TF as 2.0\n",
        "once ready reads in file and does encoding and decoding, and see model summary.\n",
        ">3 mil parameters, make sure to change GPU."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check model is okay\n",
        "Use 1 batch for this, example simple batch and pass into the mdoel\n",
        "for input_example_batch, targe_example_bacth in dataset.take 1:\n",
        "    example batch predictions = model(input_example_batch)\n",
        "example batch = original target batch is shifted +1\n",
        "Should predict 1 into the future"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "A4ygvfHn-wan"
      },
      "outputs": [],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "\n",
        "  # Predict off some random batch\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "\n",
        "  # Display the dimensions of the predictions\n",
        "  print(example_batch_predictions.shape, \" <=== (batch_size, sequence_length, vocab_size)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "example_batch_predictions.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check, 128 is batch size, 120 is seq lence of characters, 84 is vocab size\n",
        "See first 0, bunch of probabilities for each character.\n",
        "Use tf commands, tf.random.categorical, takes logartic probability, num_samples = 1.\n",
        "Want single sample, sampled_indices, get probability distribution, sampled_indices, check get ind to character, indices of predicted characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "5ld8z3LPBAuv"
      },
      "outputs": [],
      "source": [
        "example_batch_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "_achqjT-BGyY"
      },
      "outputs": [],
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "xWrPFk2nBJX4"
      },
      "outputs": [],
      "source": [
        "sampled_indices"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use tf squeeze, pass in sampled indices, axis = -1 , numpy() to reshape it, format we want it in.\n",
        "Then pass in ind_to_char dictionary index, get bunch of random characters, get prediction, random, model hasn't be trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Wi80PQVtBLqj"
      },
      "outputs": [],
      "source": [
        "# Reformat to not be a lists of lists\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4qYkIg00-wjq"
      },
      "outputs": [],
      "source": [
        "sampled_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "H9-P_XqQ_7wY"
      },
      "outputs": [],
      "source": [
        "print(\"Given the input seq: \\n\")\n",
        "print(\"\".join(ind_to_char[input_example_batch[0]]))\n",
        "print('\\n')\n",
        "print(\"Next Char Predictions: \\n\")\n",
        "print(\"\".join(ind_to_char[sampled_indices ]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DAOE4rzuBh7f"
      },
      "source": [
        "After confirming the dimensions are working, let's train our network!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Choose number of epochs, must be >=30 to get realistic and reasonable results.\n",
        "Do model.fit(dataset object, epochs= epochs)\n",
        "Fit the model, will take a long time to run, 1 - 2 min per epoch, especially for such a large mode, runtime tab, interrupt execution.\n",
        "Stop running the model, or you can wait.\n",
        "Load up the model instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ZYDQjKTlm4s8"
      },
      "outputs": [],
      "source": [
        "epochs = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "_PJ4OVdBm4s8"
      },
      "outputs": [],
      "source": [
        "model.fit(dataset,epochs=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kKkD5M6eoSiN"
      },
      "source": [
        "## Step 6: Generating text\n",
        "\n",
        "Currently our model only expects 128 sequences at a time. We can create a new model that only expects a batch_size=1. We can create a new model with this batch size, then load our saved models weights. Then call .build() on the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "eYRNG57Govdc"
      },
      "outputs": [],
      "source": [
        "model.save('shakespeare_gen.h5') "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "from tensorflow.keras.models import load_model, for Colab you must upload the model.\n",
        "Load the model weights and built another batch set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "GCoJayFS8H4d"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Final part, load up the model, load the weights and accept batch size of 1.\n",
        "Then create function to build generated text.\n",
        "\n",
        "use function create_model, pass in everything, batch_size = 1, create model with 1 batch size\n",
        "model.load_weights('previous_model.h5') - load from previous model\n",
        "Then say model.build(tf.TensorShape(1, None))\n",
        "Run 3 lines, create new instance of model, and instead of loading up full model, just load the weights with that shape size.\n",
        "Created model with 1 batch size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "_iXG3VJvEXWM"
      },
      "outputs": [],
      "source": [
        "model = create_model(vocab_size, embed_dim, rnn_neurons, batch_size=1)\n",
        "\n",
        "model.load_weights('shakespeare_gen.h5')\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "LAX3p7_YEilU"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create function to generate text\n",
        "Takes in model, start seed, gen_size, also temperature setting, set to 1.0 by default.\n",
        "Basic idea is to take some seed text, formatt it for the right shape for the network, loop the sequence as we keep adding our own predicted characters.\n",
        "Similar to time series appending predictions.\n",
        "num_gen = gen_size, num_characters to generate\n",
        "start.seed is raw string\n",
        "inp_evaluation = [char_to_und[s], for s in start_seed] - convert each character to index\n",
        "We now have to match the batch shape, so input_eval = tf.expand_dims(input_eval, 0) - expands dims and ensures correct text\n",
        "text_generated = [] to hold generated text\n",
        "temperature = temp. can make it lower or higher, so based on random or expected you want the text to be\n",
        "reset model state\n",
        "model.reset_state()\n",
        "for i in range(num_gen):\n",
        "predictions = model(input_eval)\n",
        "remove batch sha[e predictions = tf.squeeze(predictions, 0]\n",
        "predictions = predictions/temperature\n",
        "predicted_ind = tf.random.categorical(predictions, num_samples = 1) just need 1 prediction, index formatting so use [-1,0].numpy()\n",
        "input_eval = tf.expand_dims([predicted_ind])\n",
        "and transform back to characters text_generated.append(ind_to_char[predicted_ind])\n",
        "\n",
        "Check indentation and brackets, returns start seed and join all items\n",
        "return (start_seed + ''.join(text_generated))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "WvuwZBX5Ogfd"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, start_seed,gen_size=100,temp=1.0):\n",
        "  '''\n",
        "  model: Trained Model to Generate Text\n",
        "  start_seed: Intial Seed text in string form\n",
        "  gen_size: Number of characters to generate\n",
        "\n",
        "  Basic idea behind this function is to take in some seed text, format it so\n",
        "  that it is in the correct shape for our network, then loop the sequence as\n",
        "  we keep adding our own predicted characters. Similar to our work in the RNN\n",
        "  time series problems.\n",
        "  '''\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = gen_size\n",
        "\n",
        "  # Vecotrizing starting seed text\n",
        "  input_eval = [char_to_ind[s] for s in start_seed]\n",
        "\n",
        "  # Expand to match batch format shape\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty list to hold resulting generated text\n",
        "  text_generated = []\n",
        "\n",
        "  # Temperature effects randomness in our resulting text\n",
        "  # The term is derived from entropy/thermodynamics.\n",
        "  # The temperature is used to effect probability of next characters.\n",
        "  # Higher probability == lesss surprising/ more expected\n",
        "  # Lower temperature == more surprising / less expected\n",
        " \n",
        "  temperature = temp\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "\n",
        "  for i in range(num_generate):\n",
        "\n",
        "      # Generate Predictions\n",
        "      predictions = model(input_eval)\n",
        "\n",
        "      # Remove the batch shape dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # Use a cateogircal disitribution to select the next character\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # Pass the predicted charracter for the next input\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      # Transform back to character letter\n",
        "      text_generated.append(ind_to_char[predicted_id])\n",
        "\n",
        "  return (start_seed + ''.join(text_generated))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Take in model, start seed string, num characters to generate and temperature, get input evaluation, transform string back to index locations.\n",
        "transform shape for the batch, reset state of the model, generate characters, squeeze predictions to undo dimensions to grab predictions, divide by temperature to randomize or not. or /1 is leaving the same\n",
        "\n",
        "get predicted id, categorical, convert to numpy, get digits, input eval, shifted by 1,  append to predictions,convert to characters, return characters\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print, run function, pass flower, \"JULIET\", gen_size=1000 characters)), takes a while, see Shakespearan text.\n",
        "See Julliet, King Richard and other characters.\n",
        "Some words are madeup but some words make sense. Some typos, but character by character basis, see labelling of scenes, network learned structure.\n",
        "Learned characters as all CAPS and what they say, dialogue.\n",
        "Able to learn Shakespeare structure.\n",
        "Check link for more examples of RNN that generates results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "bS69SG5D5lwd"
      },
      "outputs": [],
      "source": [
        "print(generate_text(model,\"flower\",gen_size=1000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "dff5xCHIeEHO"
      },
      "outputs": [],
      "source": [
        "print(generate_text(model, \"JULIET\", gen_size=1000))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "00-Generating-Text-with-RNNs.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
